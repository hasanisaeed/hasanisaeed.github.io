---
title:  "مفهوم گرادیان نزولی تصادفی"
date: 2024-02-10T02:32:46+03:30
tags: ["گرادیان نزولی", "هوش مصنوعی", "یادگیری عمیق", "SGD", "مفاهیم"]
author: "سعید حسنی"
showToc: false
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "گرادیان نزولی تصادفی"
hideSummary: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: false
ShowRssButtonInSectionTermList: true
UseHugoToc: true
cover:
    image: "https://raw.githubusercontent.com/hasanisaeed/hasanisaeed.github.io/main/content/posts/concept-of-SGD/images/sgd.webp"
    alt: "SGD"
    caption: "این عکس با هوش مصنوعی تولید شده :)" 
    relative: true
    hidden: false
# editPost:
#     URL: "https://github.com/hasanisaeed/hasanisaeed.github.io/blob/main/content"
#     Text: "متن رو ویرایش کن 🤗"
#     appendFilePath: true # to append file path to Edit link
---
![SGD](https://raw.githubusercontent.com/hasanisaeed/hasanisaeed.github.io/main/content/posts/concept-of-SGD/images/sgd.webp#center)

### قسمت اول: درک مفاهیم پایه
#### گرادیان نزولی چیه؟
تصور کنید که شما بالا یه کوه قرار گرفتین و هدف تون این هست که میخواین به پایین ترین نقطه دست پیدا کنین. اینجاست که:
> گرادیان نزولی کمکتون میکنه تا بهترین مسیر به سمت پایین رو پیدا کنین.

زیبایی گرادیان نزولی سادگی و ظرافتش هست. توی مثال کوه مون مفهوماینه که شما با یه نقطه تصادفی روی کوه سعی میکنین که از کوه بیان پایین. در هر مرحله شما به اطراف نگاه میکنین و همش سعی میکنین که تندترین شیب (گرادیان) انتخاب کنین. این کار رو بارها و بارها تکرار میکنین. حالا چقدر؟ بهش میرسیم.

توی گرادیان نزولی هر قدم که به پایین برمیدارین، به اون قدم تون میگن اصلاحا نرخ یادگیری یا همون `learning rate`. 
> احتمالا برات سوال پیش اومده که من قدم بلند بردارم یا قدمهای کوچیک. چیزایی که الان به ذهنت میاد مفهومریاضی داره که عجله نکنی و خسته نشی برات بیشتر باز میکنم.

#### مفهوم 'تصادفی' در گرادیان نزولی تصادفی
فرض کن بالا کوه مه غلیظ گرفته و تو نمیتونی همه جا رو به خوبی ببینی(مسیله هامون توی دنیای واقعی اینجوری هستن).احتمالا قدمهای کوچیک و *تصادفی* برمیداری؟ و سعی میکنی بارها و بارها این کار رو تکرار کنی. واسه همینه که یه مسیر زیگ زاگ مانند داری همش طی میکنی!
> پس اومدیم *تصادفی* قدم برداشتیم و هر بار حواس مون هست که قدم بعدی رو محتاط تر برداریم. این همون مفهوم تصادفی بودن هست.

### قسمت دوم: ریاضیات پشت پرده SGD
#### گامهای گرادیان
- قدم اول: مقداردهی اولیه
ابتدا، پارامترها (وزن‌ها) مدل خود را مقداردهی اولیه می‌کنیم. این کار می‌تونه به صورت تصادفی یا با استفاده از روش دیگری برای مقداردهی اولیه انجام بشه. نقطه شروع برای SGD حیاتی است زیرا بر مسیر رسید به پیدا کردن نقطه بهینه تاثیر میذاره. 
- قدم دوم: انتخاب تصادفی
در هر تکرار از فرآیند آموزش، SGD به صورت تصادفی یک نقطه داده (یا یک دسته کوچک از نقاط داده) را از کل دیتاست انتخاب می‌کنه. اینکه که گفتیم تصادفی تصادفی، بخاطر این قسمت از الگورتیم بود! 
- قدم سوم: محاسبه گرادیان یا همون شیب
گرادیان تابع زیان را محاسبه کنین، اما فقط برای نقاط داده‌ای که به صورت تصادفی انتخاب شده‌اند. گرادیان یک بردار است که در جهت افزایش تندترین تابع زیان اشاره می‌کند. در مفهوم SGD، یعنی چگونه پارامترها رو برای دقیق‌تر کردن مدل برای آن نقطه داده خاص تنظیم کنیم. 
$$g_t = \nabla_\theta f_t(\theta_{t-1})$$

- قدم چهارم: آپدیت پارامترها
#### درک مفهوم نرخ یادگیری

### قسمت سوم: بریم سمت کدنویسی SGD
#### پیاده‌سازی SGD در مدل‌های یادگیری ماشین
#### پیاده‌سازی SGD در Sci-kit Learn و Tensorflow

### قسمت چهارم: مزایا و چالشهای SGD
#### چرا SGD رو انتخاب کنیم؟
#### غلبه بر چالش‌های مرتبط با SGD

###  قسمت پنجم: یه کم فراتر از مفاهیم بریم

#### انواع مختلف SGD
#### آینده‌ی SGD

### نتیجه گیری
